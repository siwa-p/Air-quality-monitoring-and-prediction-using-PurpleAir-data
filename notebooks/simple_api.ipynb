{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excellent API call code by \n",
    "## https://github.com/zfarooqui/py_purpleair_aqi/tree/main\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def get_sensors_list(nwlng, nwlat, selng, selat, location, key_read):\n",
    "    # PurpleAir API URL\n",
    "    root_url = 'https://api.purpleair.com/v1/sensors/'\n",
    "\n",
    "    # Constructing lat_lon parameters\n",
    "    lat_lon = {\n",
    "        'nwlng': nwlng,\n",
    "        'nwlat': nwlat,\n",
    "        'selng': selng,\n",
    "        'selat': selat\n",
    "    }\n",
    "    ll_api_url = ''.join([f'&{key}={value}' for key, value in lat_lon.items()])\n",
    "\n",
    "    # Fields to retrieve\n",
    "    fields_list = ['sensor_index', 'name', 'latitude', 'longitude', 'location_type']\n",
    "    fields_api_url = '&fields=' + ','.join(fields_list)\n",
    "\n",
    "    # Indoor, outdoor, or all\n",
    "    if location == 'indoor':\n",
    "        loc_api = '&location_type=1'\n",
    "    elif location == 'outdoor':\n",
    "        loc_api = '&location_type=0'\n",
    "    else:\n",
    "        loc_api = ''\n",
    "\n",
    "    # Final API URL\n",
    "    api_url = f\"{root_url}?api_key={key_read}{fields_api_url}{ll_api_url}{loc_api}\"\n",
    "\n",
    "    # Getting data\n",
    "    response = requests.get(api_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        json_data = response.json().get('data', [])\n",
    "        if json_data:\n",
    "            df = pd.DataFrame.from_records(json_data, columns=fields_list)\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=fields_list)\n",
    "    else:\n",
    "        raise requests.exceptions.RequestException(f\"Failed to fetch data from {api_url}\")\n",
    "\n",
    "    # Saving to PostgreSQL (optional)\n",
    "    # df.to_sql('tablename', con=engine, if_exists='append', index=False)\n",
    "\n",
    "    # Saving to CSV file\n",
    "    df.to_csv(\"../datasets/sensor_data/sensor_list\", index=False, header=True)\n",
    "\n",
    "    # Creating a list of sensor indices\n",
    "    sensors_list = df['sensor_index'].tolist()\n",
    "\n",
    "    return sensors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../keys.json') as fi:\n",
    "    credentials = json.load(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'api_key': 'F743EBF7-F1C6-11EE-B9F7-42010A80000D'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[133634]\n"
     ]
    }
   ],
   "source": [
    "# 35.193264659685866, -101.92062397763742\n",
    "# 35.16868927923588, -101.87431391404589\n",
    "nwlng = -101.94  # Northwest longitude of the bounding box\n",
    "nwlat = 35.20   # Northwest latitude of the bounding box\n",
    "selng = -101.87  # Southeast longitude of the bounding box\n",
    "selat = 35.16   # Southeast latitude of the bounding box\n",
    "location = 'outdoor'  # You can specify 'indoor', 'outdoor', or 'all'\n",
    "keys_file_path = '../keys.json'  # Path to your keys.json file\n",
    "\n",
    "sensors_list = get_sensors_list(nwlng, nwlat, selng, selat, location, credentials['api_key'])\n",
    "print(sensors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_index</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>location_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>133634</td>\n",
       "      <td>AMS Allergy Amarillo</td>\n",
       "      <td>0</td>\n",
       "      <td>35.173115</td>\n",
       "      <td>-101.933075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sensor_index                  name  latitude  longitude  location_type\n",
       "0        133634  AMS Allergy Amarillo         0  35.173115    -101.933075"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/sensor_data/sensor_list')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "from io import StringIO\n",
    "# from sqlalchemy import create_engine\n",
    "\n",
    "# Starting engine for postgresql\n",
    "# engine = create_engine('postgresql://postgres:password@location:port/database')\n",
    "\n",
    "# API Keys provided by PurpleAir(c)\n",
    "key_read  = credentials['api_key']\n",
    "\n",
    "# Sleep Seconds\n",
    "sleep_seconds = 3 # wait sleep_seconds after each query\n",
    "\n",
    "\n",
    "def get_historicaldata(sensors_list,bdate,edate,average_time,key_read):\n",
    "    # Historical API URL\n",
    "    root_api_url = 'https://api.purpleair.com/v1/sensors/'\n",
    "    \n",
    "    # Average time: The desired average in minutes, one of the following:0 (real-time),10 (default if not specified),30,60\n",
    "    average_api = f'&average={average_time}'\n",
    "\n",
    "    # Creating fields api url from fields list to download the data: Note: Sensor ID/Index will not be downloaded as default\n",
    "    fields_list = ['pm2.5_atm_a', 'pm2.5_atm_b', 'pm2.5_cf_1_a', 'pm2.5_cf_1_b', 'humidity_a', 'humidity_b', \n",
    "               'temperature_a', 'temperature_b', 'pressure_a', 'pressure_b']\n",
    "    for i,f in enumerate(fields_list):\n",
    "        if (i == 0):\n",
    "            fields_api_url = f'&fields={f}'\n",
    "        else:\n",
    "            fields_api_url += f'%2C{f}'\n",
    "\n",
    "    # Dates of Historical Data period\n",
    "    begindate = datetime.fromisoformat(bdate)\n",
    "    enddate   = datetime.fromisoformat(edate)\n",
    "    \n",
    "    # Downlaod days based on average\n",
    "    if (average_time == 60):\n",
    "        datelist = pd.date_range(begindate,enddate,freq='14d') # for 14 days of data\n",
    "    else:\n",
    "        datelist = pd.date_range(begindate,enddate,freq='2d') # for 2 days of data\n",
    "        \n",
    "    # Reversing to get data from end date to start date\n",
    "    datelist = datelist.tolist()\n",
    "    datelist.reverse()\n",
    "    \n",
    "    # Converting to PA required format\n",
    "    date_list=[]\n",
    "    for dt in datelist:\n",
    "        dd = dt.strftime('%Y-%m-%d') + 'T' + dt.strftime('%H:%M:%S') +'Z'\n",
    "        date_list.append(dd)\n",
    "\n",
    "    # to get data from end date to start date\n",
    "    len_datelist = len(date_list) - 1\n",
    "        \n",
    "    # Getting 2-data for one sensor at a time\n",
    "    for s in sensors_list:\n",
    "        # Adding sensor_index & API Key\n",
    "        hist_api_url = root_api_url + f'{s}/history/csv?api_key={key_read}'\n",
    "\n",
    "        # Creating start and end date api url\n",
    "        for i,d in enumerate(date_list):\n",
    "            # Wait time \n",
    "            time.sleep(sleep_seconds)\n",
    "            \n",
    "            if (i < len_datelist):\n",
    "                print('Downloading for Amarillo: %s for Dates: %s and %s.' %(s,date_list[i+1],d))\n",
    "                dates_api_url = f'&start_timestamp={date_list[i+1]}&end_timestamp={d}'\n",
    "            \n",
    "                # Final API URL\n",
    "                api_url = hist_api_url + dates_api_url + average_api + fields_api_url\n",
    "                            \n",
    "                #\n",
    "                try:\n",
    "                    response = requests.get(api_url)\n",
    "                except:\n",
    "                    print(api_url)\n",
    "                #\n",
    "                try:\n",
    "                    assert response.status_code == requests.codes.ok\n",
    "                \n",
    "                    # Creating a Pandas DataFrame\n",
    "                    df = pd.read_csv(StringIO(response.text), sep=\",\", header=0)\n",
    "                \n",
    "                except AssertionError:\n",
    "                    df = pd.DataFrame()\n",
    "                    print('Bad URL!')\n",
    "            \n",
    "                if df.empty:\n",
    "                    print('------------- No Data Available -------------')\n",
    "                else:\n",
    "                    # Dropping duplicate rows\n",
    "                    df = df.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "                    \n",
    "                    # writing to csv file\n",
    "                    filename = ('../datasets/sensor_data/Amarillo/6hrs_avg/\\sensorsID_%s_%s_%s.csv' % (s,date_list[i+1],d))\n",
    "                    df.to_csv(filename, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading for Amarillo: 133634 for Dates: 2024-03-30T00:00:00Z and 2024-04-01T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-03-28T00:00:00Z and 2024-03-30T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-03-26T00:00:00Z and 2024-03-28T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-03-24T00:00:00Z and 2024-03-26T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-03-22T00:00:00Z and 2024-03-24T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-03-20T00:00:00Z and 2024-03-22T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-03-18T00:00:00Z and 2024-03-20T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-03-16T00:00:00Z and 2024-03-18T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-03-14T00:00:00Z and 2024-03-16T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-03-12T00:00:00Z and 2024-03-14T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-03-10T00:00:00Z and 2024-03-12T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-03-08T00:00:00Z and 2024-03-10T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-03-06T00:00:00Z and 2024-03-08T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-03-04T00:00:00Z and 2024-03-06T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-03-02T00:00:00Z and 2024-03-04T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-02-29T00:00:00Z and 2024-03-02T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-02-27T00:00:00Z and 2024-02-29T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-02-25T00:00:00Z and 2024-02-27T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-02-23T00:00:00Z and 2024-02-25T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-02-21T00:00:00Z and 2024-02-23T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-02-19T00:00:00Z and 2024-02-21T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-02-17T00:00:00Z and 2024-02-19T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-02-15T00:00:00Z and 2024-02-17T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-02-13T00:00:00Z and 2024-02-15T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-02-11T00:00:00Z and 2024-02-13T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-02-09T00:00:00Z and 2024-02-11T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-02-07T00:00:00Z and 2024-02-09T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-02-05T00:00:00Z and 2024-02-07T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-02-03T00:00:00Z and 2024-02-05T00:00:00Z.\n",
      "Downloading for Amarillo: 133634 for Dates: 2024-02-01T00:00:00Z and 2024-02-03T00:00:00Z.\n"
     ]
    }
   ],
   "source": [
    "# Data download period\n",
    "bdate = '2024-02-01T00:00:00+00:00' \n",
    "edate = '2024-04-01T00:00:00+00:00'\n",
    "\n",
    "df = pd.read_csv(\"../datasets/sensor_data/sensor_list\")\n",
    "\n",
    "# Creating a list of sensor indices\n",
    "sensors_list = df['sensor_index'].tolist()\n",
    "\n",
    "# Average_time. The desired average in minutes, one of the following: 0 (real-time), \n",
    "#                  10 (default if not specified), 30, 60, 360 (6 hour), 1440 (1 day)\n",
    "average_time=360  # or 10  or 0 (Current script is set only for real-time, 10, or 60 minutes data)\n",
    "\n",
    "# Getting Nashville_area data\n",
    "get_historicaldata(sensors_list, bdate, edate, average_time, key_read)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_msdso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
