{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excellent API call code by \n",
    "## https://github.com/zfarooqui/py_purpleair_aqi/tree/main\n",
    "\n",
    "# api to get sensor list in a geometrical bounded area\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import sqlite3\n",
    "\n",
    "def get_sensors_list(nwlng, nwlat, selng, selat, location, key_read):\n",
    "    # PurpleAir API URL\n",
    "    root_url = 'https://api.purpleair.com/v1/sensors/'\n",
    "\n",
    "    # Constructing lat_lon parameters\n",
    "    lat_lon = {\n",
    "        'nwlng': nwlng,\n",
    "        'nwlat': nwlat,\n",
    "        'selng': selng,\n",
    "        'selat': selat\n",
    "    }\n",
    "    ll_api_url = ''.join([f'&{key}={value}' for key, value in lat_lon.items()])\n",
    "\n",
    "    # Fields to retrieve\n",
    "    fields_list = ['sensor_index', 'name', 'latitude', 'longitude']\n",
    "    fields_api_url = '&fields=' + ','.join(fields_list)\n",
    "\n",
    "    # Indoor, outdoor, or all\n",
    "    if location == 'indoor':\n",
    "        loc_api = '&location_type=1'\n",
    "    elif location == 'outdoor':\n",
    "        loc_api = '&location_type=0'\n",
    "    else:\n",
    "        loc_api = ''\n",
    "\n",
    "    # Final API URL\n",
    "    api_url = f\"{root_url}?api_key={key_read}{fields_api_url}{ll_api_url}{loc_api}\"\n",
    "\n",
    "    # Getting data\n",
    "    response = requests.get(api_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        json_data = response.json().get('data', [])\n",
    "        if json_data:\n",
    "            df = pd.DataFrame.from_records(json_data, columns=fields_list)\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=fields_list)\n",
    "    else:\n",
    "        raise requests.exceptions.RequestException(f\"Failed to fetch data from {api_url}\")\n",
    "\n",
    "    # Saving to sqlite (optional)\n",
    "    db = sqlite3.connect('../datasets/dallas.sqlite')\n",
    "    df.to_sql('sensor_table',\n",
    "              db, \n",
    "              if_exists='append', \n",
    "              index=False)\n",
    "    \n",
    "    db.execute('CREATE INDEX sensor_index ON sensor_table(sensor_index)')\n",
    "    db.close()\n",
    "\n",
    "    # Saving to CSV file\n",
    "    df.to_csv(\"../datasets/sensor_data/sensor_list\", index=False, header=True)\n",
    "\n",
    "    # Creating a list of sensor indices\n",
    "    sensors_list = df['sensor_index'].tolist()\n",
    "\n",
    "    return sensors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../keys.json') as fi:\n",
    "    credentials = json.load(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2644, 9504, 12969, 13013, 16271, 46221, 51821, 53365, 53389, 59801, 59903, 59907, 59953, 72071, 80533, 87019, 87485, 87721, 90785, 95481, 97395, 99159, 99163, 99187, 99279, 99309, 99585, 99593, 99595, 104402, 109566, 112984, 113642, 113648, 113708, 113833, 113857, 113969, 113975, 114119, 114329, 118993, 120681, 122927, 123021, 123409, 123453, 127045, 127049, 127059, 127067, 127075, 128645, 133820, 135002, 142154, 144032, 144468, 147038, 151486, 164335, 164965, 165171, 182041, 184053, 196323, 196421, 221467]\n"
     ]
    }
   ],
   "source": [
    "# # Bounding Box for Dallas\n",
    "# # 33.308403956633406, -97.42744748978863\n",
    "# # 32.36533339607889, -96.27246459337103\n",
    "# nwlng = -97.43  # Northwest longitude of the bounding box\n",
    "# nwlat = 33.31   # Northwest latitude of the bounding box\n",
    "# selng = -96.28  # Southeast longitude of the bounding box\n",
    "# selat = 32.37  # Southeast latitude of the bounding box\n",
    "# location = 'outdoor'  # You can specify 'indoor', 'outdoor', or 'all'\n",
    "# keys_file_path = '../keys.json'  # Path to your keys.json file\n",
    "\n",
    "# sensors_list = get_sensors_list(nwlng, nwlat, selng, selat, location, credentials['api_key'])\n",
    "# print(sensors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "average_time = 720\n",
    "bdate = '2018-04-01T00:00:00+00:00' \n",
    "edate = '2024-04-01T00:00:00+00:00'\n",
    "begindate = datetime.fromisoformat(bdate)\n",
    "enddate   = datetime.fromisoformat(edate)\n",
    "\n",
    "# Downlaod days based on average\n",
    "if (average_time == 60):\n",
    "    datelist = pd.date_range(begindate,enddate,freq='14d') # for 14 days of data\n",
    "else:\n",
    "    datelist = pd.date_range(begindate,enddate,freq='2d') # for 2 days of data\n",
    "    \n",
    "# Reversing to get data from end date to start date\n",
    "datelist = datelist.tolist()\n",
    "datelist.reverse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list=[]\n",
    "for dt in datelist:\n",
    "    dd = dt.strftime('%Y-%m-%d') + 'T' + dt.strftime('%H:%M:%S') +'Z'\n",
    "    date_list.append(dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api to get historical data for a sensor\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "from io import StringIO\n",
    "# from sqlalchemy import create_engine\n",
    "\n",
    "# Starting engine for postgresql\n",
    "# engine = create_engine('postgresql://postgres:password@location:port/database')\n",
    "\n",
    "# API Keys provided by PurpleAir(c)\n",
    "key_read  = credentials['api_key']\n",
    "\n",
    "# Sleep Seconds\n",
    "sleep_seconds = 3 # wait sleep_seconds after each query\n",
    "\n",
    "\n",
    "def get_historicaldata(sensors_list,bdate,edate,average_time,key_read):\n",
    "    # Historical API URL\n",
    "    root_api_url = 'https://api.purpleair.com/v1/sensors/'\n",
    "    \n",
    "    # Average time: The desired average in minutes, one of the following:0 (real-time),10 (default if not specified),30,60\n",
    "    average_api = f'&average={average_time}'\n",
    "\n",
    "    # Creating fields api url from fields list to download the data: Note: Sensor ID/Index will not be downloaded as default\n",
    "    fields_list = ['pm2.5_atm_a', 'pm2.5_atm_b', 'pm2.5_cf_1_a', 'pm2.5_cf_1_b', 'humidity_a', 'humidity_b', \n",
    "               'temperature_a', 'temperature_b', 'pressure_a', 'pressure_b']\n",
    "    for i,f in enumerate(fields_list):\n",
    "        if (i == 0):\n",
    "            fields_api_url = f'&fields={f}'\n",
    "        else:\n",
    "            fields_api_url += f'%2C{f}'\n",
    "\n",
    "    # Dates of Historical Data period\n",
    "    begindate = datetime.fromisoformat(bdate)\n",
    "    enddate   = datetime.fromisoformat(edate)\n",
    "    \n",
    "    # Downlaod days based on average\n",
    "    if (average_time == 60):\n",
    "        datelist = pd.date_range(begindate,enddate,freq='14d') # for 14 days of data\n",
    "    else:\n",
    "        datelist = pd.date_range(begindate,enddate,freq='2d') # for 2 days of data\n",
    "        \n",
    "    # Reversing to get data from end date to start date\n",
    "    datelist = datelist.tolist()\n",
    "    datelist.reverse()\n",
    "    \n",
    "    # Converting to PA required format\n",
    "    date_list=[]\n",
    "    for dt in datelist:\n",
    "        dd = dt.strftime('%Y-%m-%d') + 'T' + dt.strftime('%H:%M:%S') +'Z'\n",
    "        date_list.append(dd)\n",
    "\n",
    "    # to get data from end date to start date\n",
    "    len_datelist = len(date_list) - 1\n",
    "    db = sqlite3.connect('../datasets/dallas.sqlite')    \n",
    "    # Getting 2-data for one sensor at a time\n",
    "    for s in sensors_list:\n",
    "        # Adding sensor_index & API Key\n",
    "        hist_api_url = root_api_url + f'{s}/history/csv?api_key={key_read}'\n",
    "\n",
    "        # Creating start and end date api url\n",
    "        for i,d in enumerate(date_list):\n",
    "            # Wait time \n",
    "            time.sleep(sleep_seconds)\n",
    "            \n",
    "            if (i < len_datelist):\n",
    "                print('Downloading for Dallas: %s for Dates: %s and %s.' %(s,date_list[i+1],d))\n",
    "                dates_api_url = f'&start_timestamp={date_list[i+1]}&end_timestamp={d}'\n",
    "            \n",
    "                # Final API URL\n",
    "                api_url = hist_api_url + dates_api_url + average_api + fields_api_url\n",
    "                            \n",
    "                #\n",
    "                try:\n",
    "                    response = requests.get(api_url)\n",
    "                except:\n",
    "                    print(api_url)\n",
    "                #\n",
    "                try:\n",
    "                    assert response.status_code == requests.codes.ok\n",
    "                \n",
    "                    # Creating a Pandas DataFrame\n",
    "                    df = pd.read_csv(StringIO(response.text), sep=\",\", header=0)\n",
    "                \n",
    "                except AssertionError:\n",
    "                    df = pd.DataFrame()\n",
    "                    print('Bad URL!')\n",
    "            \n",
    "                if df.empty:\n",
    "                    print('------------- No Data Available -------------')\n",
    "                else:\n",
    "                    # Dropping duplicate rows\n",
    "                    df = df.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "                    # adding to sqlite\n",
    "                    df.to_sql('data_table',\n",
    "                        db, \n",
    "                        if_exists='append', \n",
    "                        index=False)\n",
    "                    \n",
    "                    # writing to csv file\n",
    "                    filename = ('../datasets/sensor_data/Dallas/\\sensorsID_%s_%s_%s.csv' % (s,date_list[i+1],d))\n",
    "                    df.to_csv(filename, index=False, header=True)\n",
    "    db.execute('CREATE INDEX IF NOT EXISTS sensor_index ON data_table(sensor_index)')\n",
    "    db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_msdso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
